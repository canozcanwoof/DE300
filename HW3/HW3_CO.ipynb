{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e247509-a79f-4b99-8902-f69a4f6fd911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import psycopg2\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, col\n",
    "from itertools import combinations\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d72658-113c-4830-b998-295615161667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         age  sex  painloc  painexer  relrest  \\\n",
      "0                                         63  1.0      NaN       NaN      NaN   \n",
      "1                                         67  1.0      NaN       NaN      NaN   \n",
      "2                                         67  1.0      NaN       NaN      NaN   \n",
      "3                                         37  1.0      NaN       NaN      NaN   \n",
      "4                                         41  0.0      NaN       NaN      NaN   \n",
      "...                                      ...  ...      ...       ...      ...   \n",
      "1056  1 888 -9 4016 8216 8216 788 0 -9 -9 -9  NaN      NaN       NaN      NaN   \n",
      "1057      -0 0 0 1 9 -9 130 80 0 130 80 0 11  NaN      NaN       NaN      NaN   \n",
      "1058                     -9 3 1h9 1 -9 -9 -9  NaN      NaN       NaN      NaN   \n",
      "1059                           -9 3 -9 -9 -9  NaN      NaN       NaN      NaN   \n",
      "1060                    -9 -9 -9 3 -9 -4 1 1  NaN      NaN       NaN      NaN   \n",
      "\n",
      "      pncaden   cp  trestbps  htn   chol  ...  exeref  exerwm  thal  thalsev  \\\n",
      "0         NaN  1.0     145.0  1.0  233.0  ...     NaN     NaN   6.0      NaN   \n",
      "1         NaN  4.0     160.0  1.0  286.0  ...     NaN     NaN   3.0      NaN   \n",
      "2         NaN  4.0     120.0  1.0  229.0  ...     NaN     NaN   7.0      NaN   \n",
      "3         NaN  3.0     130.0  0.0  250.0  ...     NaN     NaN   3.0      NaN   \n",
      "4         NaN  2.0     130.0  1.0  204.0  ...     NaN     NaN   3.0      NaN   \n",
      "...       ...  ...       ...  ...    ...  ...     ...     ...   ...      ...   \n",
      "1056      NaN  NaN       NaN  NaN    NaN  ...     NaN     NaN   NaN      NaN   \n",
      "1057      NaN  NaN       NaN  NaN    NaN  ...     NaN     NaN   NaN      NaN   \n",
      "1058      NaN  NaN       NaN  NaN    NaN  ...     NaN     NaN   NaN      NaN   \n",
      "1059      NaN  NaN       NaN  NaN    NaN  ...     NaN     NaN   NaN      NaN   \n",
      "1060      NaN  NaN       NaN  NaN    NaN  ...     NaN     NaN   NaN      NaN   \n",
      "\n",
      "      thalpul  earlobe  cmo  cday   cyr  target  \n",
      "0         NaN      NaN  2.0  16.0  81.0     0.0  \n",
      "1         NaN      NaN  2.0   5.0  81.0     1.0  \n",
      "2         NaN      NaN  2.0  20.0  81.0     1.0  \n",
      "3         NaN      NaN  2.0   4.0  81.0     0.0  \n",
      "4         NaN      NaN  2.0  18.0  81.0     0.0  \n",
      "...       ...      ...  ...   ...   ...     ...  \n",
      "1056      NaN      NaN  NaN   NaN   NaN     NaN  \n",
      "1057      NaN      NaN  NaN   NaN   NaN     NaN  \n",
      "1058      NaN      NaN  NaN   NaN   NaN     NaN  \n",
      "1059      NaN      NaN  NaN   NaN   NaN     NaN  \n",
      "1060      NaN      NaN  NaN   NaN   NaN     NaN  \n",
      "\n",
      "[1061 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "from io import BytesIO\n",
    "\n",
    "try:\n",
    "    session = boto3.Session(profile_name='canozcan') \n",
    "    s3 = session.client('s3', region_name='us-east-2', aws_access_key_id = '', \n",
    "aws_secret_access_key = 'yK6P+2oCuEIIFXaUSP3sa7wBXGqL8VuVlCumasho', aws_session_token = 'IQoJb3JpZ2luX2VjEBoaCXVzLWVhc3QtMiJIMEYCIQCYbK2hti+YbhG66jqBX4/25WJH99rld6iALzAQIcvOUgIhAJ9POy4CnLGNZdmYL6OAOau76eaxJgJpT7dSQ6C/eAgJKvQCCJP//////////wEQABoMNTQ5Nzg3MDkwMDA4IgxAFWwfZWtFebYjzbIqyAKFm+s0bzxR5MADibt5m9yr0/xUjxujm/xg6uNNugnVuoZkuRqfWFK+PGkgsz7dUHDvrjYZrNWlzxJHB0LRgqNI0mgy7x9tzguaqpDJJ6/B14EDzga18/qDHz8zVCYi0P++GgYzMZ7BLFzQU8f/vr/0DqMr8krEUBnA7B4FHKqpgGg7hbQ2wRdK5lXCmiPOh9BlXnacX6v1apN8677ssAi28H4gRwB0QG3C/m+CcvIPvCfjbboteRv+k/ueUo+hbdtGSkHq0x2IJMk4dEwtAuIJl85gGuIJ3mJ7a1hMpCIrATm8KCK0/fuj6CwuLxSIpBQQBqptGAhnLp9q36g9Vd5emrjDR4T/GLd3wemc1YV1c/Rf9VxXcZk9dYJwC5DwEArDV762W+hRQt9o54TBXIxN3iWXQC4rsfmXYwAFR1zB4OTO2bo+FUx/MOGfw7IGOqYBSdgFw4y9t9GU/1WCMKbsHrxEMVQeaAVSpLLj5T2oGRJao2ZbsvHn3XPLXQ/4gsdW7rkYZurpmlGqpU6laXhUCREQSznQMoY6kxwdkMSO7wjkT26n5aGYC482sOWTCm1zuOTCRroNiUyQHvsulDjOU+6kxi5vQ5aK1+e39bqeTV7UIrD0y1FW0jqvZukJfNshwuOUIKCBwPf+3g6LaZdoUqD138CKqg==')\n",
    "    bucket_name = 'de300spring2024'\n",
    "    object_key = 'canozcan/heart_disease.csv'\n",
    "    csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    raw_data = pd.read_csv(BytesIO(csv_string.encode()))\n",
    "    print(raw_data)\n",
    "except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "    print(f\"Credential error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d133a4-db30-4bba-addf-f4d4335dbfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 20:29:35 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://de300spring2024/canozcan/heart_disease.csv.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39mhadoopConfiguration()\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3.amazonaws.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read the CSV file from S3\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://de300spring2024/canozcan/heart_disease.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PredictingHeartDisease\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    "    .getOrCreate()\n",
    "# Set AWS credentials\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"ASIAYAAO5HRMBEYMHEHK\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"bW27sr5YoHZnNnkUqPXJB/u9GoWU40JFHCAMYq8/\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "# Read the CSV file from S3\n",
    "df = spark.read.csv('s3://de300spring2024/canozcan/heart_disease.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "178a1dda-be9a-4546-9443-f4449486a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 05:29:11 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://de300spring2024/canozcan/heart_disease.csv.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o46.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredict Heart Disease\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.375\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      7\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3a://de300spring2024/canozcan/heart_disease.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m read_data(spark, file_path)\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m clean_data(raw_data)\n\u001b[1;32m     10\u001b[0m data\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(spark, file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m(spark: SparkSession, file_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(file_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o46.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HeartDiseasePrediction\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Read, clean, and process the data\n",
    "    raw_data = read_data(spark)\n",
    "    data = clean_data(raw_data)\n",
    "    model_pipeline(data)\n",
    "    \n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f044320-9d18-4abe-8d68-ab9f836de348",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datafolder = \"../data\"\n",
    "\n",
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \n",
    "    data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(Datafolder,\"*.csv\"))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31b66d-d18c-4010-a9bc-4cbc09e2e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_cut(data: DataFrame) -> DataFrame:\n",
    "    columns_left = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke','fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "    less_data = data.select(columns_left)\n",
    "    return less_data\n",
    "    \n",
    "def replace_outofbound(data: DataFrame) -> DataFrame:\n",
    "    data = data.withColumn('painloc', when(col('painloc') < 0, 0).when(col('painloc') > 1, 1).otherwise(col('painloc')))\n",
    "    data = data.withColumn('painexer', when(col('painexer') < 0, 0).when(col('painexer') > 1, 1).otherwise(col('painexer')))\n",
    "    data = data.withColumn('trestbps', when(col('trestbps') < 100, 100).otherwise(col('trestbps')))\n",
    "    data = data.withColumn('oldpeak', when(col('oldpeak') < 0, 0).when(col('oldpeak') > 4, 4).otherwise(col('oldpeak')))\n",
    "    data = data.withColumn('fbs', when(col('fbs') < 0, 0).when(col('fbs') > 1, 1).otherwise(col('fbs')))\n",
    "    data = data.withColumn('prop', when(col('prop') < 0, 0).when(col('prop') > 1, 1).otherwise(col('prop')))\n",
    "    data = data.withColumn('nitr', when(col('nitr') < 0, 0).when(col('nitr') > 1, 1).otherwise(col('nitr')))\n",
    "    data = data.withColumn('pro', when(col('pro') < 0, 0).when(col('pro') > 1, 1).otherwise(col('pro')))\n",
    "    data = data.withColumn('diuretic', when(col('diuretic') < 0, 0).when(col('diuretic') > 1, 1).otherwise(col('diuretic')))\n",
    "    data = data.withColumn('exang', when(col('exang') < 0, 0).when(col('exang') > 1, 1).otherwise(col('exang')))\n",
    "    data = data.withColumn('slope', when(col('slope') < 1, None).when(col('slope') > 3, None).otherwise(col('slope')))\n",
    "    return data\n",
    "    \n",
    "def replace_null_to_mean(data: DataFrame) -> DataFrame:\n",
    "    columns = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "    \n",
    "    for column in columns:\n",
    "        mean_value = data.select(.mean(col(column))).collect()[0][0]\n",
    "        if mean_value is not None:\n",
    "            data = data.withColumn(column, when(col(column).isNull(), mean_value).otherwise(col(column)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bf48d-8d5a-4637-ae4a-0dd45dced657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoke1(data: DataFrame) -> DataFrame:\n",
    "    url1 = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release'\n",
    "    response = requests.get(url1)\n",
    "    html_content = response.content\n",
    "    full_sel = Selector(text=html_content)\n",
    "    tables = full_sel.xpath('//table')\n",
    "    smokers_by_age = tables[1]\n",
    "    rows = smokers_by_age.xpath('./tbody//tr')\n",
    "\n",
    "    def parse1(row:Selector) -> List[str]:\n",
    "     \n",
    "        cells = row.xpath('.//th | .//td')\n",
    "        row_data = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            if i == 0 or i == 10:\n",
    "                cell_text = cell.xpath('normalize-space(.)').get()\n",
    "                cell_text = re.sub(r'<.*?>', ' ', cell_text) \n",
    "                cell_text = cell_text.replace('\\xa0', '') \n",
    "                row_data.append(cell_text)\n",
    "        \n",
    "        return row_data\n",
    "    \n",
    "    table_data = [parse1(row) for row in rows]\n",
    "\n",
    "    def rate1(age):\n",
    "        try:\n",
    "            age = int(age)\n",
    "            for i, row in enumerate(table_data):\n",
    "                if i < len(table_data) - 1:\n",
    "                    cutoff = row[0].split('–')[1]\n",
    "                    if age <= int(cutoff):\n",
    "                        return float(row[1])\n",
    "                else:\n",
    "                    return float(row[1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    rate1_udf = F.udf(lambda age: rate1(age) / 100, DoubleType())\n",
    "    data = data.withColumn('smoke1', when(col('smoke1').isNull(), get_rate_1_udf(col('age'))).otherwise(col('smoke1')))\n",
    "    return data\n",
    "\n",
    "def smoke2(data: DataFrame) -> DataFrame:\n",
    "    url2 = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
    "    response = requests.get(url2)\n",
    "\n",
    "    selector = Selector(text=response.content)\n",
    "\n",
    "    ul_sel_list = selector.xpath('//ul[@class=\"block-list\"]')\n",
    "    genders = ul_sel_list[0]\n",
    "    ages = ul_sel_list[1]\n",
    "\n",
    "    def gender_percentage_cleaned(rows):\n",
    "        dict = {}\n",
    "        for row in rows:\n",
    "            gender = 'woman' if 'women' in row.split('(')[0] else 'man'\n",
    "            percent = float(row.split('(')[1].split('%')[0])\n",
    "            dict[gender] = float(percent)\n",
    "        return dict\n",
    "\n",
    "    def age_percentage_cleaned(rows):\n",
    "        for i, row in enumerate(rows):\n",
    "            if i < len(rows) - 1:\n",
    "                age = int(row.split('–')[1].split(' ')[0])\n",
    "            else:\n",
    "                age = int(row.split(' ')[7])\n",
    "                \n",
    "            percent = float(row.split('(')[1].split('%')[0])\n",
    "            rows[i] = [age, percent]\n",
    "        return rows\n",
    "\n",
    "    def parse2(row:Selector) -> List[str]:\n",
    "      \n",
    "        cells = row.xpath('./li')\n",
    "        row_data = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            cell_text = cell.xpath('normalize-space(.)').get()\n",
    "            cell_text = re.sub(r'<.*?>', ' ', cell_text) \n",
    "            cell_text = cell_text.replace('\\xa0', '') \n",
    "            row_data.append(cell_text)\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    percent_gender = gender_percentage_cleaned(parse2(genders))\n",
    "    percent_age = age_percentage_cleaned(parse2(ages))\n",
    "\n",
    "    def rate2(sex, age):\n",
    "        if sex == 0:\n",
    "            try:\n",
    "                age = int(age)\n",
    "                for i, row in enumerate(percent_age):\n",
    "                    if i < len(percent_age) - 1:\n",
    "                        if age <= row[0]:\n",
    "                            return row[1]\n",
    "                    else:\n",
    "                        return row[1]\n",
    "            except:\n",
    "                return np.nan\n",
    "        else:\n",
    "            try:\n",
    "                age = int(age)\n",
    "                for i, row in enumerate(percent_age):\n",
    "                    if i < len(percent_age) - 1:\n",
    "                        if age <= row[0]:\n",
    "                            return row[1] * percent_gender['man'] / percent_gender['woman']\n",
    "                    else:\n",
    "                        return row[1] * percent_gender['man'] / percent_gender['woman']\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "\n",
    "    rate2_udf = F.udf(lambda sex, age: rate2(sex, age) / 100, DoubleType())\n",
    "\n",
    "    data = data.withColumn('smoke2', when(col('smoke2').isNull(), get_rate_2_udf(col('sex'), col('age'))).otherwise(col('smoke2')))\n",
    "\n",
    "    return data \n",
    "\n",
    "def smoke_fix(data: DataFrame) -> DataFrame:\n",
    "    data = data.withColumn('smoke1', F.col('smoke'))\n",
    "    data = data.withColumn('smoke2', F.col('smoke'))\n",
    "\n",
    "    data = smoke1(data)\n",
    "    data = smoke2(data)\n",
    "\n",
    "    data = data.drop('smoke')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36607d06-1e08-4b89-9e66-eecdfc837d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: DataFrame) -> DataFrame:\n",
    "    data = column_cut(data)\n",
    "    data = replace_outofbound(data)\n",
    "    data = replace_null_to_mean(data)\n",
    "    data = smoke_fix(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d19c3a-e38a-4df8-9aa8-f02af50c6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(data: DataFrame):\n",
    "\n",
    "    num_folds = 4\n",
    "    random_seed = 7580\n",
    "    train_test_ratio = 0.85\n",
    "    \n",
    "    # Drop null targets\n",
    "    data = data.dropna(subset=['target'])\n",
    "\n",
    "    # Convert age to integer\n",
    "    data = data.withColumn(\"age\", data[\"age\"].cast(IntegerType()))\n",
    "\n",
    "    num_features = [f.name for f in data.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "    all_features = [f.name for f in data.schema.fields]\n",
    "\n",
    "    imputed_cols = [f\"Imp{v}\" for v in num_features]\n",
    "    imputer = Imputer(inputCols=num_features, outputCols=imputed_cols, strategy=\"mean\")\n",
    "\n",
    "    # Assemble feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_cols, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Define Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline_stages = Pipeline(stages=[imputer, assembler, rf_classifier])\n",
    "\n",
    "    # Param grid for max tree depth and number of trees\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf_classifier.maxDepth, [2, 4, 6, 8, 10]) \\\n",
    "        .addGrid(rf_classifier.numTrees, [150, 200, 250, 500]) \\\n",
    "        .build()\n",
    "\n",
    "    # Cross-validator setup\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline_stages,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        seed=random_seed)\n",
    "\n",
    "    # Train-test split\n",
    "    train_data, test_data = data.randomSplit([train_test_ratio, 1-train_test_ratio], seed=random_seed)\n",
    "\n",
    "    # Train the cross-validated model\n",
    "    cv_model = crossval.fit(train_data)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = cv_model.transform(test_data)\n",
    "\n",
    "    # Evaluate model\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Best model from cross-validation\n",
    "    best_rf_model = cv_model.bestModel.stages[-1]\n",
    "\n",
    "    # Print selected max tree depth\n",
    "    max_depth = best_rf_model.getOrDefault(best_rf_model.getParam(\"maxDepth\"))\n",
    "    print(f\"Selected Max Depth: {max_depth}\")\n",
    "\n",
    "    # Print selected number of trees\n",
    "    num_trees = best_rf_model.getOrDefault(best_rf_model.getParam(\"numTrees\"))\n",
    "    print(f\"Selected Num Trees: {num_trees}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
